{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn off logging\n",
    "import logging, sys\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hdbscan\n",
    "import operator\n",
    "import operator\n",
    "import pylab\n",
    "import json\n",
    "import spacy\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from itertools import combinations\n",
    "from nltk import Tree\n",
    "from nltk.corpus import wordnet as wn\n",
    "from pytrips.ontology import load\n",
    "ont = load()\n",
    "en_nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector file \n",
    "with open(\"./data/countries.json\", \"r\") as file:\n",
    "    countryFile = file.read()\n",
    "countries = json.loads(countryFile)\n",
    "vectorDir = \"/Users/aeshaanwahlang/Documents/QuantitativeSemantics/NounVectors/\"\n",
    "file = \"nounVectorFull.csv\"\n",
    "vectors = pd.read_csv(vectorDir+file)\n",
    "data = pd.DataFrame(vectors)\n",
    "dataz = vectors.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10093, 249)"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove useless columns and normalization (min-max)\n",
    "def clean(df):\n",
    "    data = df.copy()\n",
    "    data = data.drop(columns=['Noun', 'ROOT;', 'punct;'])\n",
    "    scaler = MinMaxScaler() \n",
    "    scaled_values = scaler.fit_transform(data) \n",
    "    data.loc[:,:] = scaled_values\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in vectors.columns:\n",
    "#     print(\"\\\"\"+c+\"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clustering parameters\n",
    "metric = 'braycurtis'\n",
    "min = 3\n",
    "# initialize clusterer\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size = min, metric = metric, prediction_data=True,\n",
    "                           cluster_selection_method = \"leaf\", alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class definition\n",
    "class Cluster:\n",
    "    @staticmethod\n",
    "    def getAgreement(tType, data):\n",
    "        c = 0\n",
    "        found = False\n",
    "        for word in data:\n",
    "            for t in word.tType['lex']:\n",
    "                if t == tType:\n",
    "                    c += 1\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                for t in word.tType['wn']:\n",
    "                    if t == tType:\n",
    "                        c += 1\n",
    "                        break\n",
    "            else: found = False\n",
    "                \n",
    "        return c/len(data)\n",
    "    \n",
    "    def __init__(self, tType, content):\n",
    "        self.__words = []\n",
    "        self.tType = ont.get_trips_type(tType)\n",
    "        for w in content:\n",
    "            self.__words.append(Word(w))\n",
    "        self.agreement = Cluster.getAgreement(self.tType, self.__words)\n",
    "        self.size = len(self.__words)\n",
    "        \n",
    "    def __str__(self):\n",
    "        print(\"-----------------\")\n",
    "        print(\"TRIPS type: \" + str(self.tType))\n",
    "        print(\"Agreement: \" + str(self.acceptance))\n",
    "        print(\"Size: \" + str(self.size))\n",
    "        print(\"\")\n",
    "        for w in self.__words: print(w)\n",
    "        return \"-----------------\"\n",
    "\n",
    "    def to_dict(self):\n",
    "        dump = {}\n",
    "        dump['trips'] = str(self.tType)\n",
    "        dump['agreement'] = self.agreement\n",
    "        dump['size'] = self.size\n",
    "        dump['words'] = []\n",
    "        for w in self.__words: dump['words'].append(str(w))\n",
    "        \n",
    "        return dump\n",
    "    \n",
    "class Word:\n",
    "    tType = None\n",
    "    string = None\n",
    "    \n",
    "    def __init__(self, string):\n",
    "        self.string = string\n",
    "        self.tType = tripsType(string)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.string\n",
    "        \n",
    "\n",
    "def organizeClusters(df):\n",
    "    clustering = []\n",
    "    group = list(df['Noun'].groupby(df[\"TRIPS_Node\"]))\n",
    "    for g in group:\n",
    "        clustering.append(Cluster(g[0], g[1]))\n",
    "    return clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clutering results\n",
    "# find tripsType with PyTrips\n",
    "def tripsType(word):\n",
    "    if not isinstance(word, str): return (\"N/A\", [])\n",
    "    ontType = ont[(\"q::\"+word, \"n\")]\n",
    "    country = countryCheck(word)\n",
    "    if country: ontType[\"lex\"].append(ont.get_trips_type(\"ont::geographic-region\"))\n",
    "    return ontType\n",
    "\n",
    "# count frequencies to dic\n",
    "def countToDic(dic, val):\n",
    "    if val in dic: dic[val] += 1\n",
    "    else: dic[val] = 1\n",
    "\n",
    "# check if word is a country\n",
    "def countryCheck(word):\n",
    "    for c in countries:\n",
    "        if c['name']['common'].lower() == word: return \"ont::geographic-region\"\n",
    "        if c['name']['official'].lower() == word: return \"ont::geographic-region\"\n",
    "        if c['region'].lower() == word: return \"ont::geographic-region\"\n",
    "        alt = list(map(str.lower,c['altSpellings']))\n",
    "        if word in alt: return \"ont::geographic-region\"\n",
    "         \n",
    "# find frequencies \n",
    "def findFreq(freqDict, nodes):\n",
    "    l = 0\n",
    "    for node in nodes:\n",
    "        if isinstance(node, list):\n",
    "            for n in node:\n",
    "                countToDic(freqDict, n)\n",
    "                l +=1\n",
    "        else:\n",
    "            countToDic(freqDict, node)\n",
    "            l +=1\n",
    "    for k in freqDict: freqDict[k] = freqDict[k]/l\n",
    "    return\n",
    "\n",
    "# Assign TRIPS node to cluster\n",
    "def clusterTtype(cluster):\n",
    "    freq = {}\n",
    "    \n",
    "    def addToDFreq(tType):\n",
    "        resnicPath = [(2**-i, j) for i, j in enumerate(tType.path_to_root())]\n",
    "        for node in resnicPath:\n",
    "            sNode = str(node[1])\n",
    "            if sNode in freq: freq[sNode] += node[0]\n",
    "            else: freq[sNode] = node[0]\n",
    "    \n",
    "    for word in cluster:\n",
    "        ontTypes = tripsType(word)\n",
    "        for tType in ontTypes[\"lex\"]: addToDFreq(tType)\n",
    "        for tType in ontTypes[\"wn\"]: addToDFreq(tType)\n",
    "    \n",
    "    if not freq: return \"N/A\"\n",
    "    val = max(freq.items(), key=operator.itemgetter(1))[0]\n",
    "#     print(freq)\n",
    "    return val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster details\n",
    "def showCluster_details(clusters):\n",
    "    res = clusters.labels_\n",
    "    unique, counts = np.unique(res, return_counts = True)\n",
    "    uni = dict(zip(unique, counts))\n",
    "    # print(uni)\n",
    "    del uni[-1] #remove noise labels\n",
    "    max = 0\n",
    "    total = 0\n",
    "    for key in uni:\n",
    "        val = uni[key]\n",
    "        if(val > max):\n",
    "            max = val\n",
    "        total += val\n",
    "    sorted_uni = sorted(uni.items(), key=operator.itemgetter(1))\n",
    "    print(\"Clusters found: \" + str(res.max()+1))\n",
    "    noise = np.count_nonzero(res == -1)\n",
    "    noise2 = round(noise/len(res), 3)*100\n",
    "    print(\"Noise: \" + str(noise) + \" (\" + str(noise2) + \"%)\")\n",
    "    print(\"Avarage cluster size: \" + str(round(total/len(uni), 3)) + \"\\nLargest cluster: \" + str(max))\n",
    "#     print(\"10 largest clusters:\")\n",
    "#     for i in range(len(sorted_uni)):\n",
    "#         print(sorted_uni[len(sorted_uni)-(1+i)])\n",
    "#         if(i > 10):\n",
    "#             break\n",
    "            \n",
    "        \n",
    "\n",
    "# format output string for cluster results\n",
    "def outStr(row, showTypes):\n",
    "    out = row[0] + \" \" * (20-len(row[0]))\n",
    "    out = out + str(row[1]) + \" \" * (5-len(str(row[1])))\n",
    "    out += str(row[2]) + \" \" *(20 - len(str(row[2])))\n",
    "    if showTypes: out += row[3]\n",
    "    return out\n",
    "\n",
    "def showClusters(dataz, showTypes=False):\n",
    "    if showTypes: nf = dataz.loc[dataz['cluster'] >= 0].sort_values(['cluster'])[['Noun','cluster', 'TRIPS_Node', 'TRIPS_type']]\n",
    "    else: nf = dataz.loc[dataz['cluster'] >= 0].sort_values(['cluster'])[['Noun','cluster', 'TRIPS_Node']]\n",
    "    print(\"Noun \\t\\t Cluster \\t TRIPS Node \\t\\t TRIPS Type\")\n",
    "    print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "    c = 0\n",
    "    for row in nf.iterrows():\n",
    "        if row[1][1] != c:\n",
    "            print(\"-----------------------\")\n",
    "            c = row[1][1]            \n",
    "        print(outStr(row[1], showTypes))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign clusters and TRIPS type to each cluster\n",
    "def assignClusters(df, res):\n",
    "    df['TRIPS_type'] = df['Noun'].apply(tripsType)\n",
    "    for i in range(0, res.max()+1):\n",
    "        cluster = df.loc[df[\"cluster\"]==i, \"Noun\"].values\n",
    "        df.loc[df.cluster ==  i, \"TRIPS_Node\"] = clusterTtype(cluster)\n",
    "\n",
    "def clusterAnalysis(clustering):\n",
    "    size = 0\n",
    "    agg = 0\n",
    "    agg05 = 0\n",
    "    agg03 = 0\n",
    "    agg07 = 0\n",
    "    l = len(clustering)\n",
    "    for c in clustering:\n",
    "        size += c.size\n",
    "        agg += c.agreement\n",
    "        if c.agreement >= 0.5: agg05 += 1\n",
    "        if c.agreement >= 0.3: agg03 += 1\n",
    "        if c.agreement >= 0.7: agg07 += 1\n",
    "    \n",
    "    print(\"Number of TRIPS types: \" + str(l))\n",
    "    print(\"Avg size: \" + str(size/l))\n",
    "    print(\"Avg Agreement: \" + str(agg/l))\n",
    "    print(\"Number of Agreement >= 70%: \" + str(agg07))\n",
    "    print(\"Number of Agreement >= 50%: \" + str(agg05))\n",
    "    print(\"Number of Agreement >= 30%: \" + str(agg03))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find words that TRIPS dosent assign\n",
    "def findUnassigned(words):\n",
    "    unassigned = []\n",
    "    for word in words:\n",
    "        tType = tripsType(word, returnDict=True)\n",
    "        if not isinstance(tType, dict):\n",
    "            continue\n",
    "        if tType[\"lex\"] == [] and tType[\"wn\"] == []:\n",
    "            if countryCheck(word) == None: unassigned.append(word)\n",
    "    return unassigned\n",
    "\n",
    "# returns how many words in a cluster actually belong to a TRIPS type\n",
    "def getClusterConsistency(cluster, tType):\n",
    "    found = 0\n",
    "    for word in cluster:\n",
    "        cType = tripsType(word, returnDict=True)\n",
    "        if tType in cType[\"lex\"] or tType in cType[\"wn\"]: found += 1\n",
    "    return round(found/len(cluster), 4)\n",
    "\n",
    "# returns the % of words found in the word closure of the TRIPS type\n",
    "def getWordClosureCount(cluster, closure):\n",
    "    count = 0\n",
    "    for c in cluster:\n",
    "        if c in closure: count+=1\n",
    "    return count/len(closure)\n",
    "\n",
    "# return avg % of words in TRIPS type word closure for a group\n",
    "def groupAnalysis(group, show = False, get = False):\n",
    "    leaves = {}\n",
    "    parents = 0\n",
    "    \n",
    "    for g in group:\n",
    "        if g[0] == \"None\": continue\n",
    "        if g[0] == \"ont::geographic-region\":\n",
    "            parents += 1\n",
    "            continue\n",
    "            \n",
    "        words = g[0].word_closure()\n",
    "        tType = str(g[0])\n",
    "        if words:\n",
    "            leaves[tType] = {}\n",
    "            leaves[tType]['cluster_consistency'] = getClusterConsistency(g[1], g[0])\n",
    "            leaves[tType]['word_closure'] = getWordClosureCount(g[1], words)\n",
    "            leaves[tType]['cluster_size'] = len(g[1])\n",
    "        else: parents += 1\n",
    "            \n",
    "    if show:\n",
    "        l = len(leaves)\n",
    "        print(\"Number of Parent nodes: \"+ str(parents))\n",
    "        print(\"Number of Leaf nodes: \"+ str(l))\n",
    "        avgCC = 0\n",
    "        avgWC = 0\n",
    "        avgSize = 0\n",
    "        for key,val in leaves.items():\n",
    "            avgCC += val['cluster_consistency']\n",
    "            avgWC += val['word_closure']\n",
    "            avgSize += val['cluster_size']\n",
    "        print(\"Avg Cluster Consistency: \"+str(round(avgCC/l,4)))\n",
    "        print(\"Avg Word Closure:\" + str(round(avgWC/l,4)))\n",
    "        print(\"Avg Cluster Size: \" + str(round(avgSize/l, 4)))\n",
    "        \n",
    "    if get:\n",
    "        return (leaves, parents)\n",
    "\n",
    "    \n",
    "# find how many clusters have trips_nodes \n",
    "def showClusterTRIPSanalysis(data):\n",
    "    size = 0\n",
    "    for c in data:\n",
    "        group = list(c[1].groupby(c[1]))\n",
    "        if c[0] == \"None\":\n",
    "            print(\"Number of clusters with no TRIPS Node:\" + str(len(group)))\n",
    "        else: size += len(group)\n",
    "    print(\"Total merged cluster:\" + str(size))\n",
    "    print(\"Avg number of cluster merged: \"+ str(round(size/len(data), 2)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to run clustering and return cluster obj\n",
    "def makeClusters(data, clusterer, threshold):\n",
    "    size = data.shape[0]\n",
    "    #backup may not need if all goes well\n",
    "    dataz = data.copy()\n",
    "    combined = None\n",
    "    count = 1\n",
    "    initial = True\n",
    "    predictionData = []\n",
    "    \n",
    "    #initial clustering \n",
    "    cleanedData = clean(dataz)\n",
    "    clusters = clusterer.fit(cleanedData)\n",
    "    dataz['cluster'] = clusterer.labels_\n",
    "    assignClusters(dataz, clusterer.labels_)\n",
    "    print(\"Clusterings done: \" + str(count) + \" size:\" + str(size))\n",
    "\n",
    "    while(size > threshold):\n",
    "        noise = dataz[dataz[\"cluster\"] < 0]\n",
    "        cleanNoise = noise.drop(columns=['Noun', 'cluster', 'TRIPS_type', 'TRIPS_Node'])\n",
    "        noiseClusters = clusterer.fit(cleanNoise)\n",
    "        noise['cluster'] = clusterer.labels_\n",
    "        count += 1\n",
    "        print(\"Clusterings done: \" + str(count) + \" size:\" + str(cleanNoise.shape[0]))\n",
    "        \n",
    "        assignClusters(noise, clusterer.labels_)\n",
    "        g2 = noise[noise['cluster'] >= 0]\n",
    "        \n",
    "        if initial:\n",
    "            g1 = dataz[dataz['cluster'] >= 0]\n",
    "            combined = g1.append(g2)\n",
    "            initial = False\n",
    "        else:\n",
    "            combined = combined.append(g2)\n",
    "        \n",
    "        dataz = noise\n",
    "        size = dataz.shape[0]\n",
    "        print(\"combined: \" + str(combined.shape[0]))\n",
    "        \n",
    "        \n",
    "    print(\"Completed\")\n",
    "    print(\"leftovers: \" + str(size))\n",
    "    final = organizeClusters(combined)\n",
    "    return (final, combined)\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = makeClusters(data, clusterer, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of TRIPS types: 336\n",
      "Avg size: 26.988095238095237\n",
      "Avg Agreement: 0.4031633691030206\n",
      "Number of Agreement >= 70%: 20\n",
      "Number of Agreement >= 50%: 100\n",
      "Number of Agreement >= 30%: 261\n"
     ]
    }
   ],
   "source": [
    "# group = list(res[1]['Noun'].groupby(res[1][\"TRIPS_Node\"]))\n",
    "clusterAnalysis(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters found: 555\n",
      "Noise: 7621 (75.5%)\n",
      "Avarage cluster size: 4.454\n",
      "Largest cluster: 25\n"
     ]
    }
   ],
   "source": [
    "# Run clustring \n",
    "cleanedData = clean(data)\n",
    "clusters = clusterer.fit(cleanedData)\n",
    "# predic_data = clusters.prediction_data_\n",
    "showCluster_details(clusters)\n",
    "# assignClusters(dataz, clusters.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "dataz['cluster'] = clusterer.labels_\n",
    "dataClustered = dataz.loc[dataz['cluster'] >= 0]\n",
    "assignClusters(dataClustered, clusters.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test prediction\n",
    "iv = data.loc[data['Noun'] == \"bat\"]\n",
    "iv = iv.drop(columns=['Noun', 'ROOT;', 'punct;'])\n",
    "test = hdbscan.approximate_predict(clusterer, iv)\n",
    "# print(iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/anaconda3/lib/python36.zip', '/anaconda3/lib/python3.6', '/anaconda3/lib/python3.6/lib-dynload', '/anaconda3/lib/python3.6/site-packages', '/anaconda3/lib/python3.6/site-packages/aeosa', '/anaconda3/lib/python3.6/site-packages/IPython/extensions', '/Users/aeshaanwahlang/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-1]), array([0.]))\n",
      "3664    ont::day-name\n",
      "5757    ont::day-name\n",
      "7838    ont::day-name\n",
      "8841    ont::day-name\n",
      "9178    ont::day-name\n",
      "9446    ont::day-name\n",
      "9860    ont::day-name\n",
      "Name: TRIPS_Node, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(dataClustered.loc[dataClustered['cluster'] == 82, 'TRIPS_Node'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns combination of relelvant dependencies of word in sentence\n",
    "def depParse(sentence, word):\n",
    "    doc = en_nlp(sentence)\n",
    "    word = word.lower()\n",
    "    deps =[]\n",
    "    combi = []\n",
    "    for token in doc:\n",
    "        if token.text.lower() == word or token.head.text.lower() == word:\n",
    "            deps.append(token.dep_+\";\")\n",
    "    for i in range(0, len(deps)+1):\n",
    "        for subset in combinations(deps, i):\n",
    "            combi.append(subset)\n",
    "    return combi\n",
    "\n",
    "# returns instance vector\n",
    "def getInstanceVector(word, deps, df):\n",
    "    with open('vector_map_full.pkl', 'rb') as f:\n",
    "        vmap = pickle.load(f)\n",
    "    print(len(vmap))\n",
    "    wordVec = df.loc[df[\"Noun\"] == word.lower()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object saved\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n"
     ]
    }
   ],
   "source": [
    "d = depParse(\"The quick brow fox jumped over the lazy dog\", \"fox\")\n",
    "getInstanceVector(\"test\", d, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = organizeClusters(res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = {}\n",
    "for clus in r1:\n",
    "    dump[str(clus.tType)] = clus.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = json.dumps(dump)\n",
    "with open('/Users/aeshaanwahlang/Desktop/ClustersDump.json', \"w\") as f:\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7621, 252)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recluster Noise\n",
    "# noise = dataz.copy()\n",
    "# noise = noise[noise[\"cluster\"] < 0]\n",
    "# noise2 = noise.drop(columns=['Noun', 'cluster', 'TRIPS_type', 'TRIPS_Node'])\n",
    "# noise_clusters = clusterer.fit(noise2)\n",
    "# cluster_details(noise_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assignClusters(noise, noise_clusters.labels_)\n",
    "# showClusters(noise, showTypes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assignedData1 = dataz[dataz[\"cluster\"] >= 0]\n",
    "# assignedData2 = noise[noise['cluster'] >= 0]\n",
    "# combinedData = assignedData1.append(assignedData2)\n",
    "# mergedClusters = Clusters(combinedData)\n",
    "# groups = combinedData['Noun'].groupby(combinedData[\"TRIPS_Node\"])\n",
    "# clustesToTripsGroups = combinedData['cluster'].groupby(combinedData['TRIPS_Node'])\n",
    "# groups = list(groups)\n",
    "# g2 = list(clustesToTripsGroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mergedClusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(276, 1063    276\n",
      "7015    276\n",
      "7602    276\n",
      "Name: cluster, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "x = g2[0][1]\n",
    "y = list(x.groupby(x))\n",
    "print(y[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parent nodes: 20\n",
      "Number of Leaf nodes: 160\n",
      "Avg Cluster Consistency: 0.5118\n",
      "Avg Word Closure:0.0371\n",
      "Avg Cluster Size: 14.1625\n",
      "*****\n",
      "Number of clusters with no TRIPS Node:323\n",
      "Total merged cluster:600\n",
      "Avg number of cluster merged: 3.31\n"
     ]
    }
   ],
   "source": [
    "# merged cluster analysis\n",
    "groupAnalysis(groups, show=True)\n",
    "print(\"*****\")\n",
    "showClusterTRIPSanalysis(g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #print Trees and poking around with clustrer\n",
    "# plot = clusterer.condensed_tree_.plot(select_clusters=False, selection_palette=sns.color_palette())\n",
    "# # plot = clusterer.single_linkage_tree_.plot(cmap='viridis', colorbar=True)\n",
    "# # plt.switch_backend('QT4Agg')\n",
    "# # figManager = plt.get_current_fig_manager()\n",
    "# # figManager.window.showMaximized()\n",
    "# plt.savefig(\"singleLinlkageTree.png\", dpi = 500)\n",
    "# plt.show()\n",
    "# # fig = plot.get_fi \n",
    "\n",
    "# # clusterer.minimum_spanning_tree_.plot(edge_cmap='viridis',\n",
    "# #                                       edge_alpha=0.2,\n",
    "# #                                       node_size=8,\n",
    "# #                                       edge_linewidth=1)\n",
    "\n",
    "# # G = clusterer.condensed_tree_.to_networkx()\n",
    "# # num = clusterer.condensed_tree_.to_numpy()\n",
    "# # predicData = clusterer.prediction_data_.cluster_map\n",
    "# # cTree = clusterer.prediction_data_.cluster_tree\n",
    "# # print(predicData)\n",
    "# # print(\"------------\")\n",
    "# # print(cTree)\n",
    "\n",
    "# # for k in predic_data.keys():\n",
    "# #     n = data.loc[]\n",
    "# #     print(n)\n",
    "# #     break\n",
    "\n",
    "# # indeg = G.in_degree()\n",
    "# # # print(G.edges)\n",
    "# # nx.draw_networkx(G)\n",
    "# # mng = plt.get_current_fig_manager()\n",
    "# # mng.frame.Maximize(True)\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save clustring results \n",
    "# #We should maybe standardize this output also\n",
    "# # dataz = dataz.loc[:, dataz.columns.isin(['Noun', 'cluster'])]\n",
    "# test = True\n",
    "# if(test):\n",
    "#     saveName =  'TestResults/HDBSCAN_' + met + '_' + str(min) +'('+file+')'+ '.csv'\n",
    "# else:\n",
    "#     saveName = 'ClusteringResults/HDBSCAN_' + met + '_' + str(min) +'('+file+')'+ '.csv'\n",
    "# dataz.to_csv(saveName, sep=',', encoding='utf-8', index=False)\n",
    "# # print(dataz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Soft Clustring\n",
    "# vec = hdbscan.all_points_membership_vectors(clusterer)\n",
    "# vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save Soft Clustring results as Membership Probability\n",
    "\n",
    "# cols = [str(x) for x in range(vec.shape[1])]\n",
    "# mem_prob = pd.DataFrame(0, index = np.arange(vec.shape[0]), columns=cols)\n",
    "# mem_prob['Noun'] = dataz['Noun']\n",
    "# count = 0\n",
    "# for row in vec:\n",
    "#     for col in cols:\n",
    "#         mem_prob.loc[count,col] = row[int(col)]\n",
    "#     count +=1\n",
    "# mem_prob.to_csv('Membership_Probability.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mem_prob\n",
    "# print(predic_data.cluster_map)\n",
    "# outliers = clusterer.outlier_scores_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_obj(obj, name ):\n",
    "#     with open('./'+ name + '.pkl', 'wb') as f:\n",
    "#         pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "#     print(\"Object saved\")\n",
    "# save_obj(vdict, \"vector_map_full\")\n",
    "\n",
    "# vmap = [\"compound;\",\"punct;\",\"dep;\",\"case;\",\"appos;\",\"nsubj;\",\"compound;punct;\",\"compound;dep;\",\"nmod:of;\",\"det:the;\",\"amod;\",\"conj:and;\",\"dobj;\",\"nmod:other;\",\"nmod:poss;\",\"case;nmod:of;\",\"det:the;nmod:of;\",\"case;det:the;\",\"amod;nmod:of;\",\"conj:and;nmod:of;\",\"dobj;nmod:of;\",\"case;nmod:other;\",\"case;compound;\",\"det:the;nsubj;\",\"nmod:for;\",\"ROOT;\",\"det:a/an;\",\"dep;punct;\",\"ROOT;punct;\",\"ROOT;dep;\",\"ROOT;dep;punct;\",\"case;nmod:poss;\",\"nmod:in;\",\"nmod:on;\",\"nmod:other;nmod:poss;\",\"amod;case;\",\"case;nmod:in;\",\"case;nmod:other;nmod:poss;\",\"cc;\",\"cop;\",\"amod;nsubj;\",\"det:a/an;nsubj;\",\"det:a/an;punct;\",\"nsubj;punct;\",\"cop;det:a/an;\",\"cop;punct;\",\"cop;nsubj;\",\"ROOT;nsubj;\",\"ROOT;cop;\",\"det:a/an;nsubj;punct;\",\"cop;det:a/an;punct;\",\"cop;det:a/an;nsubj;\",\"cop;nsubj;punct;\",\"ROOT;nsubj;punct;\",\"ROOT;cop;nsubj;\",\"ROOT;cop;punct;\",\"ROOT;cop;nsubj;punct;\",\"acl:to;\",\"dobj;nmod:poss;\",\"det:the;dobj;\",\"amod;dobj;\",\"amod;nmod:poss;\",\"nmod:of;nsubj;\",\"nmod:of;nmod:other;\",\"det:the;nmod:other;\",\"case;nmod:for;\",\"case;det:the;nmod:of;\",\"det:the;nmod:of;nmod:other;\",\"det:the;dobj;nmod:of;\",\"amod;det:a/an;\",\"case;det:a/an;\",\"det:a/an;dobj;\",\"amod;case;nmod:of;\",\"nmod:in;nmod:of;\",\"det:the;nmod:in;\",\"case;nmod:in;nmod:of;\",\"case;det:the;nmod:in;\",\"det:the;nmod:of;nsubj;\",\"case;conj:and;\",\"case;cc;\",\"cc;conj:and;\",\"case;cc;conj:and;\",\"appos;punct;\",\"case;det:the;nmod:other;\",\"det:a/an;nmod:of;\",\"amod;det:the;\",\"amod;nmod:other;\",\"det:a/an;nmod:other;\",\"case;nmod:of;nmod:other;\",\"case;det:a/an;nmod:other;\",\"compound;det:the;\",\"compound;nmod:of;\",\"compound;nmod:other;\",\"case;compound;det:the;\",\"case;compound;nmod:of;\",\"case;compound;nmod:other;\",\"case;nmod:on;\",\"det:the;nmod:on;\",\"case;det:the;nmod:on;\",\"nmod:with;\",\"amod;nmod:with;\",\"case;nmod:with;\",\"det:a/an;nmod:with;\",\"amod;case;det:a/an;\",\"amod;case;nmod:with;\",\"nmod:to;\",\"compound;det:a/an;\",\"amod;conj:and;\",\"case;nmod:to;\",\"mwe;\",\"nmod:poss;nsubj;\",\"dep;nsubj;\",\"compound;nmod:poss;\",\"nummod;\",\"case;nummod;\",\"dobj;nummod;\",\"nmod:other;nummod;\",\"det:a/an;nmod:for;\",\"compound;nsubj;\",\"compound;dobj;\",\"compound;conj:and;\",\"acl:relcl;\",\"ref;\",\"det:the;punct;\",\"amod;punct;\",\"acl:relcl;ref;\",\"amod;case;det:the;\",\"conj:and;dobj;\",\"amod;compound;\",\"det:a/an;nmod:to;\",\"det:the;nmod:for;\",\"case;det:the;nmod:of;nmod:other;\",\"conj:and;det:the;\",\"appos;compound;\",\"amod;nmod:in;\",\"amod;case;nmod:in;\",\"det:a/an;nmod:in;\",\"case;det:a/an;nmod:in;\",\"amod;det:a/an;dobj;\",\"ROOT;det:a/an;\",\"ROOT;det:a/an;nsubj;\",\"ROOT;det:a/an;punct;\",\"ROOT;cop;det:a/an;\",\"cop;det:a/an;nsubj;punct;\",\"ROOT;det:a/an;nsubj;punct;\",\"ROOT;cop;det:a/an;nsubj;\",\"ROOT;cop;det:a/an;punct;\",\"ROOT;cop;det:a/an;nsubj;punct;\",\"compound;nmod:in;\",\"case;compound;nmod:in;\",\"advmod;\",\"cop;det:the;\",\"amod;case;nmod:other;\",\"nmod:of;nmod:poss;\",\"case;punct;\",\"nmod:of;punct;\",\"cop;nmod:of;\",\"det:a/an;nmod:of;nsubj;\",\"amod;det:a/an;nmod:of;\",\"cop;nmod:of;nsubj;\",\"det:a/an;dobj;nmod:of;\",\"appos;det:a/an;\",\"ROOT;nmod:of;\",\"ROOT;nmod:of;punct;\",\"amod;det:the;nmod:of;\",\"det:the;nmod:to;\",\"amod;cop;\",\"cop;det:the;nmod:of;\",\"cop;det:the;nsubj;\",\"nummod;punct;\",\"det:the;nmod:poss;\",\"case;det:the;nmod:poss;\",\"case;det:a/an;nmod:of;\",\"case;mwe;\",\"case;det:a/an;nmod:with;\",\"nmod:in;nmod:poss;\",\"case;nmod:in;nmod:poss;\",\"neg;\",\"case;nsubj;\",\"case;cop;\",\"case;cop;nsubj;\",\"case;det:the;nmod:to;\",\"amod;cop;nsubj;\",\"nmod:poss;punct;\",\"dep;nummod;\",\"nsubjpass;\"]\n",
    "# vdict = {}\n",
    "# for val in vmap:\n",
    "#     vdict[val] = True\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
